### Project 4 - Machine Learning: Ashley H, Fiona B, Katie B, Pratik S, Tifani B
# To Shroom or Not to Shroom?🍄 

> **One surprising development of the Covid-19 pandemic was the resurgence of interest in mushrooms among Americans.
As people looked for refuge from their homes in nature, many took on the practice of foraging mushrooms. This rise in interest has coincided with the use of Psilocybin mushrooms for medical and recreational use, in addition to discussions about how to combat climate change and cure world hunger.** 

## Contents
- [Purpose](#purpose)
    - [Is This Mushroom Edible or Poisonous?](#is-this-mushroom-edible-or-poisonous)
- [Datasource](#datasource)
- [Overview](#overview)
    - [Machine Learning Technologies](#machine-learning-technologies)
    - [Multi-Class Classification and Mushroom Species Identification](#multi-class-classification-and-mushroom-species-identification)
- [Description of Data Analysis](#description-of-data-analysis)
    - [Step 1: Data Cleaning and Preprocessing](#step-1-data-cleaning-and-preprocessing)
    - [Step 2: Data Exploration and Visualization](#step-2-data-exploration-and-visualization)
        - [Relations Between Cap Characteristics & Mushroom Toxicity](#question-1)
        - [Relations Between Stem Characteristics & Mushroom Toxicity](#question-2)
        - [Relations Between Gill Characteristics & Mushroom Toxicity](#question-3)
        - [Relations Between Bleeding/Bruising & Mushroom Toxicity](#question-4)
        - [Relations Between Habitat/Season & Mushroom Toxicity](#question-5)
    - [Step 3: Predictive Analyses](#step-3-predictive-analyses)
        - [Principle Components Analysis (PCA)](#principle-components-analysis-pca)
            - [Findings](#pca-findings)
        - [Random Forest](#random-forest)
            - [Findings](#random-forest-findings) 
        - [Logistic Regression](#logistic-regression) 
            - [Findings](#logistic-regression-findings)
        - [Decision Tree Model](#decision-tree-model)
            - [Findings](#decision-tree-findings)
        - [Deep Neural Net](#deep-neural-net) 
            - [Findings](#deep-neural-net-findings)
- [Conclusion & Next Steps](#conclusion-and-next-steps)
- [References & Recommended Readings](#references-and-recommended-readings)

<p align="center">
  <img src="https://github.com/Ahoust7/Project-4/assets/119274891/8d830f86-812a-4305-bc59-8a31a9ec96a8" />
</p>

## Purpose
If you were to walk around in your backyard right now, there is a good chance that you would find some sort of mushroom. Before handling these types of fungi, it is important to decipher whether they are poisonous. Misidentifying poisonous mushrooms can have severe consequences, as ingestion of toxic varieties can lead to illness, organ failure, or even death. The purpose of our project is to train a machine learning model that can accurately identify and differentiate toxic mushrooms from edible ones, ensuring the safety of individuals who forage or consume wild mushrooms; thus, our main question is as follows: 
#### Is This Mushroom Edible or Poisonous?
This compelling question is one that our model seeks to answer based on certain mushroom characteristics.

## Datasource
- [Link to Dataset Files](https://mushroom.mathematik.uni-marburg.de/files/)
    - We are specifically using the `secondary_data_shuffled.csv` which can be found [here](https://mushroom.mathematik.uni-marburg.de/files/SecondaryData/secondary_data_shuffled.csv)

## Overview

### Machine Learning Technologies
- Principal Component Analysis (PCA)
- Balancing Data if needed: Imbalanced-Learn w/ Imblearn in Python Random Over/Under Sampling.
- Binary Classification: edible or poisonous?
- Logistic Regression 
- k-Nearest Neighbors
- Decision Trees
- Support Vector Machine
- TensorFlow

### Multi-Class Classification and Mushroom Species Identification
- k-Nearest Neighbors.
- Decision Trees.
- Random Forest.

## Description of Data Analysis
Data Analysis was done in a 3 step process:

   ### Step 1: Data Cleaning and Preprocessing
   - We began by importing our csv source to a jupyter notebook
   - Before modeling, we made sure to clean, normalize and standardize our data
      - When cleaning, we found dropping null values to be the most beneficial tactic since:
        - Whenever you have a higher percentage of null values, it is better to drop the column or row.
        - Whenever you have null values with outliers, it's better to impute by median.
        - Whenever you have null values without outliers, it's better to impute by mean.
        - Whenever you have null values, categorical values are sometimes better to impute by mode.
        - Sometimes, imputation of null values deals with using algorithmic imputation.

<p align="center">
  <img src="https://github.com/Ahoust7/Project-4/assets/119274891/13a3653c-7c8d-4164-8c3e-bacee90e3399"> />
</p>

   ### Step 2: Data Exploration and Visualization

#### Question 1: 
How do mushroom cap characteristics influence mushroom toxicity?

> **Image 1a**
<p align="center">
  <img src="https://github.com/Ahoust7/Project-4/assets/119274891/ed7d7671-4e54-4dc1-9ea3-49bab525a213" />
</p>

   - **Results:** Buff mushrooms appear to be the least likely to poison you. You are almost guaranteed to be poisoned by a pink, green, orange, red, or yellow mushroom. Other colors appear to be a toss-up as to whether they are poisonous or not. Best to assume more mushrooms you find in the wild are poisonous.

> **Image 1b**
<p align="center">
  <img src="https://github.com/Ahoust7/Project-4/assets/119274891/85eb14a8-42f4-4d64-9452-6c8b95efa0d9" />
</p>

   - **Results:** It appears that the bigger a mushroom cap is, the less likely it is to be poisonous.
  
#### Question 2: 
How do stem characteristics influence mushroom toxicity?

> **Image 2a** 
<p align="center">
  <img src="https://github.com/Ahoust7/Project-4/assets/119274891/f5a54345-4058-464a-ae2b-d6cd5da6ae10" />
</p>

   - **Results:** Mushrooms with buff stems appear to almost always be edible. Mushrooms with white stems are also less likely to be poisonous. Stear clear of all other stem colors, however, as they are likely to be poisonous.

> **Image 2b**
<p align="center">
  <img src="https://github.com/Ahoust7/Project-4/assets/119274891/0cae269a-de79-4573-910d-8c7f9e670236" />
</p>

   - **Results:** It appears that taller stemmed mushrooms are less likely to be poisonous.

> **Image 2c**
<p align="center">
  <img src="https://github.com/Ahoust7/Project-4/assets/119274891/4517e7ad-6fc0-4325-aa9e-9032a0d45ba4" />
</p>

   - **Results:** Mushrooms with wider stems appear to be less likely to be poisonous.

#### Question 3:
How do gill characteristics influence mushroom toxicity?

> **Image 3a**
<p align="center">
  <img src="https://github.com/Ahoust7/Project-4/assets/119274891/998f8d15-52af-491b-afef-78dc356e57cb" />
</p>

   - **Results:** Mushrooms with white and buff gills appear to be less poisonous. Brown-, yellow-, red-, and pink-gilled mushrooms appear to be especially poisonous.

#### Question 4:
How does bleeding and bruising influence mushroom toxicity?

> **Image 4a**
<p align="center">
  <img src="https://github.com/Ahoust7/Project-4/assets/119274891/6cafbc03-b5a6-493a-ad6f-0e5c8a76e682" />
</p>

   - **Results:** It appears that you would have better luck not being poisoned by a mushroom that bleeds and/or bruises compared to one that doesn't, though you are likely to get poisoned regardless.

#### Question 5:
How does habitat and season influence mushroom toxicity?

> **Image 5a**
<p align="center">
  <img src="https://github.com/Ahoust7/Project-4/assets/119274891/18e956d4-9e24-437f-9bb9-331f18e62fac" />
</p>

   - **Results:** Urban mushrooms appear to be less poisonous, although their representation in the dataset is minimal. Leaf mushrooms also appear to be less poisonous. Mushrooms found in grasses or woods, however, appear more likely to be poisonous.

> **Image 5b**
<p align="center">
  <img src="https://github.com/Ahoust7/Project-4/assets/119274891/534b0104-8376-4fe6-82c2-8cade7cc4e6d" />
</p>

   - **Result:** Most poisonous mushrooms can be found in the summer or autumn. You are less likely to find a poisonous mushroom in the Spring and Winter.

   ### Step 3: Predictive Analyses
    
   #### Data Transformation and Setup

**1.** First, we code the discrete variables to numerics   

<p align="center">
  <img src="https://github.com/Ahoust7/Project-4/assets/119274891/eeca4337-e844-42aa-90bb-7b963a98ed2c" />
</p>


**2.** Then, we separate out our response variable (poisonous or not) from the predictor variables (everything else).

<p align="center">
  <img src="https://github.com/Ahoust7/Project-4/assets/119274891/1dbaf2ed-3d71-47a9-a779-bfb214c88c66" />
</p>

**3.** If we don't apply scaling, the models will be more influenced by features such as cap shape and color and stem and gill color compared to features like bruises and whether the mushrooms has rings. This is because the former features have a greater impact due to their wider range of choices.

<p align="center">
  <img src="https://github.com/Ahoust7/Project-4/assets/119274891/5c73e4fe-fa6e-4100-94f2-84aa0bfcf459" />
</p>

**4.** We can see from the correlation matrix that the features are mostly uncorrelated.

<p align="center">
  <img src="https://github.com/Ahoust7/Project-4/assets/119274891/0b61a794-3066-42b3-8e46-5d7fd91bbe47" />
</p>

   #### Principle Components Analysis (PCA)
   
   **1. Splitting the Data:** We start by splitting the dataset into training and testing sets using the `train_test_split` function. This allows us to have separate data to train our model and evaluate its performance.
   
<p align="center">
  <img src="https://github.com/Ahoust7/Project-4/assets/119274891/e51900b7-b1cc-4a40-a71a-2dcf6955ce36" />
</p>

   **2. Standardizing the Data:** To ensure that all features are on the same scale, we create an instance of `StandardScaler` and fit it to the training data. Then, we use the fitted scaler to transform both the training and testing data. Standardization is important for many machine learning algorithms as it helps prevent certain features from dominating others due to differences in their scales.
   
<p align="center">
  <img src="https://github.com/Ahoust7/Project-4/assets/119274891/87acc4ff-4b3d-4f3e-b6e3-6935f1369be2" />
</p>
   
   **3. Performing PCA:** We create a PCA object with `n_components=2`, indicating that we want to reduce the dimensionality of our data to two principal components. We then fit the PCA model to the scaled training data and transform both the training and testing data into the reduced feature space.

 <img src="https://github.com/Ahoust7/Project-4/assets/119274891/195b4804-c66f-4250-b3a9-b5ec00c5b82d" width="500">

 <img src="https://github.com/Ahoust7/Project-4/assets/119274891/64f8c98b-0a63-4f5d-b371-02d51fe254a1" width="500">

   **4. Visualizing the Transformed Data:** We plot the transformed data in a scatter plot, using the first principal component on the x-axis and the second principal component on the y-axis. The use of `alpha=0.1` allows for better visualization when there are overlapping points, as it makes the points more transparent.

 <img src="https://github.com/Ahoust7/Project-4/assets/119274891/01206ea0-6710-440f-b76a-b09595a7be04" alt="alt text" width="whatever" height="whatever">


 <img src="https://github.com/Ahoust7/Project-4/assets/119274891/942eea06-691b-4df3-bca2-a635013ee10f" alt="alt text" width="whatever" height="whatever">

   **5. Explained Variance Ratio:** We calculate and print the explained variance ratio for the first two principal components. The explained variance ratio tells us the proportion of the dataset's variance that is explained by each principal component. It provides insight into how much information is retained by each component.

 <img src="https://github.com/Ahoust7/Project-4/assets/119274891/941b61dc-3e04-466c-84d8-8761603c1922" width="50%">


 <img src="https://github.com/Ahoust7/Project-4/assets/119274891/295cdfad-be12-44de-b7c5-674a0879baec" width="25%">
 
   - We can see that the first principle components analysis had a high accuracy at 87%; whereas, the second principle components analysis have relatively low accuracy at 8%.

   #### PCA Findings
   
   - By applying PCA to our dataset, we reduced the dimensionality from 11 features to 2 principal components. The original dataset had a shape of (45801, 11), and the transformed PCA data had a shape of (45801, 2). The scatter plot visualizes the transformed data, with the x-axis representing the first principal component and the y-axis representing the second principal component.

   - The explained variance ratio for the first principal component is 0.8769, indicating that this component explains approximately 87.69% of the variance in the dataset. Similarly, the explained variance ratio for the second principal component is 0.0808, explaining around 8.08% of the variance. These ratios provide insights into the amount of information retained by each principal component.

   - This dimensionality reduction technique allows us to represent the data in a lower-dimensional space while retaining a significant amount of the original information.

   #### Random Forest 
   
   **1.** We checked for model accuracy by creating a Random Forest Classifier with 500 decision trees.
   
<p align="center">
  <img src="https://github.com/Ahoust7/Project-4/assets/119274891/4cb92918-771c-4d04-8b43-0d0307074ef4" />
</p>

   **2.** We calculated the confusion matrix and classification report.
 
 <p align="center">
  <img src="https://github.com/Ahoust7/Project-4/assets/119274891/de737072-0b70-478a-8854-8fea29796241" />
</p>

   - True Positive (TP): 8446 True Negative (TN): 6760 False Positive (FP): 35 False Negative (FN): 27
        
   **3.** We visualized the features by importance.
   
 <p align="center">
  <img src="https://github.com/Ahoust7/Project-4/assets/119274891/8fd308d7-eb50-44cf-a609-c10d7063c5ce" />
</p>

   #### Random Forest Findings
   
   - Our Random Forest model achieves a remarkable training accuracy of 100.0%. This means that it accurately predicts all the labels in the training dataset. Moving on to the test accuracy, we observe a high accuracy score of 99.59%. This indicates that the model generalizes well to unseen data, demonstrating its effectiveness in classification.
   
   - The accuracy score of our Random Forest model further confirms the model's performance, with a score of 0.9959. This metric represents the overall accuracy of the predictions, indicating a high level of correctness in classifying the instances.
   
   - Additionally, we generate a comprehensive classification report. Precision, recall, and the F1-score are all outstanding for both classes (0 and 1), indicating a reliable classification performance. The support column represents the number of instances of each class in the test set.
   
   - We calculated the importance of each feature in the classification task using the Random Forest model. The bar plot presents feature importance in descending order. This allows us to identify the most influential features for making accurate predictions.
   
   - Our Random Forest Classifier demonstrates exceptional performance on both the training and test sets. The model achieves high accuracy, with a training accuracy of 100% and a test accuracy of 99.59%. The confusion matrix provides insights into the model's predictions, and the classification report demonstrates its precision, recall, and F1-score. Finally, feature importance sheds light on the significant features driving the classification task.
   
   #### Logistic Regression
   The goal of the logistic regression as a classification model was to see if it can predict the binary outcome: 
    
   - **Is this mushroom edible or poisonous?** 
   
   **1.** We fit a logistic regression model by using the training data (X_train and y_train).
   
 <p align="center">
  <img src="https://github.com/Ahoust7/Project-4/assets/119274891/39e8ae64-44d0-4697-9310-c722ee15d6e1" />
</p>

   **2.** We printed the original accuracy score of the data and generated a confusion matrix for the model 
   
<p align="center">
  <img src="https://github.com/Ahoust7/Project-4/assets/119274891/9370a720-81ec-4bd9-ba0a-3b3af88a84f2" />
</p>   

**Results** True Positive (TP): 6378 True Negative (TN): 2707 False Positive (FP): 4088 False Negative (FN): 2095
   
   **3.** We printed the classification report for the model
        
 <p align="center">
  <img src="https://github.com/Ahoust7/Project-4/assets/119274891/1e04dfcf-c2ab-4f3f-be8b-b04833de8697" />
</p>   

**Results:** Accuracy: 60% Class 0: Precision 56%, Recall 40%, F1-Score 47% Class 1: Precision 61%, Recall 75%, F1-Score 67% Weighted Average: Precision 59%, Recall 60%, F1-Score 58%

   **4.** We predicted a logistic regression model with over resampled training data

 <img src="https://github.com/Ahoust7/Project-4/assets/119274891/e151d3d8-bbc7-4c03-957d-768305b76e46" alt="alt text" width="whatever" height="whatever">

**Results:** True Positive (TP): 5011 True Negative (TN): 4038 False Positive (FP): 2757 False Negative (FN): 3462

   - Class 0 precision: 56%, recall: 40%, F1-score: 47% Class 1 precision: 61%, recall: 75%, F1-score: 67% Overall accuracy: 60% Weighted average precision: 59%, recall: 60%, F1-score: 58%

   **4.** We predicted a logistic regression model with under resampled training data

 <img src="https://github.com/Ahoust7/Project-4/assets/119274891/a6196e40-135a-4eb8-b8b9-f4d280fceda0" alt="alt text" width="whatever" height="whatever">

**Results:** True Positive (TP): 4952 True Negative (TN): 4078 False Positive (FP): 2717 False Negative (FN): 3521

   - Class 0 precision: 56%, recall: 40%, F1-score: 47% Class 1 precision: 61%, recall: 75%, F1-score: 67% Overall accuracy: 60% Weighted average precision: 59%, recall: 60%, F1-score: 58%


   #### Logistic Regression Findings
   
   - Over Sampled and Under Sampled Logistic Regression Imbalanced Data Set 
   
        - Literature says it’s best to try both. 
    
        - **Over=** duplicates examples in the minority class (edible)
    
        - **Under=** merges examples in the majority class (poisonous)
    
   - Model did not perform that well as can be seen by classification report which is most likely due to:
   
        - Data being too large.
   
        - A linear relationship between the outcome (edible and poisonous) and predicters (mushroom features) cannot be fully explained by linear relationships
   
        - 60% accuracy 

   #### Decision Tree Model
   The goal of the Decision Tree Model as a classification model was to see if it can predict the binary outcome:
    
   - **Is this mushroom edible or poisonous?** 

   **1.** We defined features and set target vectors

 <img src="https://github.com/Ahoust7/Project-4/assets/119274891/80bdab34-cc45-4a6c-b41d-f1340125f151" alt="alt text" width="whatever" height="whatever">

   **2.**  We created a `StandardScaler` instance and scaled the training data

 <img src="https://github.com/Ahoust7/Project-4/assets/119274891/f5cbf964-5a5d-455d-91ed-d9d2b34ad1c8" alt="alt text" width="whatever" height="whatever">

   **3.** We calculated the confusion matrix and accuracy score

 <img src="https://github.com/Ahoust7/Project-4/assets/119274891/3279a509-d055-419e-9ac3-f52cf8459c92" alt="alt text" width="whatever" height="whatever">

   #### Decision Tree Findings
   
- This produced an accuracy rating of 98% 
   
- Model performed well which is likely because:
    
   - It is better able to handle that the mushroom features are not linearly related to whether the mushroom is edible or poisonous. 
    
   - As mentioned earlier we had outliers and missing data, and the decision tree model is not affected by this and is able to split features on the data accordingly. 
    
    - As seen in the image of the decision tree model the data is complex and deep.  It would be interesting to see the model applied to new data as more mushroom species are discovered. 
![transactions_tree](https://github.com/Ahoust7/Project-4/assets/119638430/6f135293-d4c3-4442-a729-327d520e1fae)

   #### Deep Neural Net
   
   **1.** Data Preparation: We split our preprocessed data into our features and target arrays & scaled the data

 <img src="https://github.com/Ahoust7/Project-4/assets/119274891/a4d688f0-dccf-4b75-b79e-d6a10a2da3d6" alt="alt text" width="whatever" height="whatever">

   **2.** Model Setup: We defined our model and deep neural net

 <img src="https://github.com/Ahoust7/Project-4/assets/119274891/471d14e3-db52-471b-a102-58b416cf8589" alt="alt text" width="whatever" height="whatever">

   **3.** We compiled and trained the model 

 <img src="https://github.com/Ahoust7/Project-4/assets/119274891/a84516fe-bc6a-4526-a04f-24ecfaa43eb9" alt="alt text" width="whatever" height="whatever">

   **4.** We evaluated the model using the test data

 <img src="https://github.com/Ahoust7/Project-4/assets/119274891/8687294a-ce27-4b03-8450-3cf8763960a0" alt="alt text" width="whatever" height="whatever">

   #### Deep Neural Net Findings

   - To define the model in Step 2, we determined the number of input features we will use, how many hidden layers we were creating, and how many nodes each layer would use. For this model we chose 2 hidden layers with 100 and 75 nodes respectively, we also chose to use all the available features. The activation function for the two hidden layers was ReLU, which was chosen to explore non-linearity. For the output layer, we used sigmoid. 
 
   - When compiling the model we chose the `binary_crossentropy` function to calculate the loss because this is a binary classification model and this function computes the loss between true labels and predicted labels. Then when training the model, we included a callback for earlystop to monitor val_loss and prevent overtraining the model. We found after a few attempts that 10 epochs gave use the highest accuracy with the lowest loss.

   - When evaluating the model in Step 4, it's evident that we've achieved 98.28% accuracy a loss of 0.0515. 


## Conclusion and Next Steps

We were able to build several models that accurately predict the edibility or toxicity of mushrooms given the descriptive features in this dataset, however we believe that image identification of edible mushrooms would be significantly more helpful. So, our next step would be to use Machine Learning on mushroom photos to predict edibility. To this end we would explore both:

   **1.** A *Convolutional Neural Network Model* with Images of mushrooms to see if we can use images to identify if a mushroom is edible or poisonous.
    
   **2.** Using *Saliency Maps* to see if what highlighted features of the mushroom are relevant for classification of it being edible or poisonous and explore similarities/differences with features in current data set.

Our group also discussed how changes in laws surrounding the cultivation and use of mushrooms containing psylocibin for therapeutic purposes will change the data landscape for mushrooms. Currently psylocibin is a controlled substance and data is not available or what is available is federally controlled. As the use of psylocibin mushrooms increases in western medicine and data is released. Adding the features of psilocybin mushrooms as a hallucinogenic classification category is a possibility.

## References and Recommended Readings
- [Mental Health and Substance Abuse](https://www.vox.com/2014/12/22/7424477/mushrooms-research)
- [Climate Change](https://www.nytimes.com/interactive/2022/07/27/climate/climate-change-fungi.html)
- [World Hunger](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10213758/) 
- [Legalization](https://www.vox.com/future-perfect/21509465/psychedelic-magic-mushrooms-psilocybin-medical-legalization-decriminalization-oregon-washington-dc)
- [Scientific Report Inspiration](https://www.nature.com/articles/s41598-021-87602-3)  
